# TRL Trainer Implementation Deep Dive

**Created**: 2025-11-18
**Last Updated**: 2025-11-27
**Purpose**: Comprehensive analysis and annotation of TRL (Transformer Reinforcement Learning) library trainers for alignment techniques

---

## ğŸ“š What's in This Directory?

This directory contains heavily annotated implementations, mathematical references, and practical guides for understanding and using the TRL library's alignment trainers. **9 trainers fully documented** with complete 3-file sets each, covering **95%+ of real-world use cases**.

### âœ… Fully Documented Trainers (9)
- **SFTTrainer** - Supervised fine-tuning (foundation)
- **DPOTrainer** - Direct preference optimization
- **PPOTrainer** - Proximal policy optimization (classic RLHF)
- **GRPOTrainer** - Group relative policy optimization
- **RewardTrainer** - Reward model training
- **KTOTrainer** - Kahneman-Tversky optimization (binary preferences)
- **ORPOTrainer** - Odds ratio preference optimization (no reference model)
- **RLOOTrainer** - REINFORCE leave-one-out (RL with variance reduction)
- **CPOTrainer** - Contrastive preference optimization (flexible loss variants)

ğŸ“Š **Total**: 29 files, ~13,400 lines of documentation

ğŸ“‹ **Status**: See [STATUS.md](STATUS.md) for detailed completion tracking

---

## ğŸš€ Quick Start - Choose Your Path

### Path 1: Complete Beginner (Never trained an LLM)
**Time: ~2 hours**

1. **Start**: [00_TRAINER_CATALOG.md](00_TRAINER_CATALOG.md) (10 min)
   - Understand what each trainer does
   - See comparison table

2. **Learn SFT**: [SFTTrainer_ANNOTATED.md](annotated_trainers/SFTTrainer_ANNOTATED.md) (30 min)
   - Foundation of all alignment
   - How supervised fine-tuning works

3. **Try it**: [sft_training_ANNOTATED.py](annotated_examples/sft_training_ANNOTATED.py) (Run it!)
   - Complete working example
   - Start with small model

### Path 2: Have Preference Data (Most Common)
**Time: ~3 hours**

1. **Understand**: [RLHF_DPO_Guide.md](reference_guides/RLHF_DPO_Guide.md) (20 min)
2. **Math**: [DPO_Loss_Reference.md](reference_guides/DPO_Loss_Reference.md) (30 min)
3. **Code**: [DPOTrainer_ANNOTATED.md](annotated_trainers/DPOTrainer_ANNOTATED.md) (45 min)
4. **Run**: [dpo_training_ANNOTATED.py](annotated_examples/dpo_training_ANNOTATED.py)

### Path 3: Want RL with Rewards
**Time: ~2-3 hours**

Pick based on complexity:
- **Classic RL**: [PPOTrainer_ANNOTATED.md](annotated_trainers/PPOTrainer_ANNOTATED.md)
- **Simpler RL**: [RLOOTrainer_ANNOTATED.md](annotated_trainers/RLOOTrainer_ANNOTATED.md)
- **No Reference**: [GRPOTrainer_ANNOTATED.md](annotated_trainers/GRPOTrainer_ANNOTATED.md)

### Path 4: Just Want to Run Something Now
**Time: 10 minutes**

```bash
# Install
pip install trl transformers datasets

# Run SFT (simplest)
python annotated_examples/sft_training_ANNOTATED.py

# Or DPO (if you have preference data)
python annotated_examples/dpo_training_ANNOTATED.py
```

All scripts have sensible defaults and work out-of-the-box!

---

## ğŸ¯ Which Trainer Should I Use?

```
Do you have instruction data?
â”œâ”€ YES â†’ Start with SFTTrainer
â””â”€ NO â†’ Get some first!

After SFT, do you want to align with preferences?
â”œâ”€ Have pairs (chosen vs rejected)?
â”‚  â”œâ”€ Want simplest â†’ DPOTrainer â­ Most Popular
â”‚  â”œâ”€ Want to save memory â†’ ORPOTrainer
â”‚  â””â”€ Want flexible losses â†’ CPOTrainer
â”‚
â”œâ”€ Have binary feedback (ğŸ‘/ğŸ‘)?
â”‚  â””â”€ Use KTOTrainer
â”‚
â””â”€ Have reward function/model?
   â”œâ”€ Want best results â†’ PPOTrainer
   â”œâ”€ Want simpler â†’ RLOOTrainer
   â””â”€ Want memory efficient â†’ GRPOTrainer
```

**Still unsure?** See [00_TRAINER_CATALOG.md](00_TRAINER_CATALOG.md) for detailed comparison.

---

## ğŸ“ What's in Each File Type?

### `annotated_trainers/*.md` - Code Walkthroughs
**What**: Line-by-line explanation of trainer implementations
**When**: You want to understand HOW it works internally
**Example**: See how DPO computes loss with 15+ variants

### `reference_guides/*.md` - Math & Concepts
**What**: Mathematical derivations and theoretical foundations
**When**: You want to understand WHY it works
**Example**: Complete DPO derivation from Bradley-Terry model

### `annotated_examples/*.py` - Ready-to-Run Scripts
**What**: Production training code with extensive comments
**When**: You want to RUN training now
**Example**: Full DPO training with hyperparameter tuning

---

## ğŸ“Š Trainer Comparison

| Trainer | Data | Ref Model | Complexity | Memory | When to Use |
|---------|------|-----------|------------|--------|-------------|
| **SFT** | Instructions | âŒ | â­ | â­ | Always start here |
| **DPO** | Pairs | âœ… | â­â­ | â­â­â­ | Most popular |
| **ORPO** | Pairs | âŒ | â­â­ | â­â­ | Save memory |
| **KTO** | Binary | âœ… | â­â­ | â­â­â­ | Thumbs up/down data |
| **CPO** | Pairs | âŒ | â­â­ | â­â­ | Experiment with losses |
| **PPO** | Reward | âœ…+Value | â­â­â­ | â­â­â­â­ | Best quality |
| **RLOO** | Reward | Optional | â­â­ | â­â­ | Simpler than PPO |
| **GRPO** | Reward | âŒ | â­â­ | â­ | Memory efficient RL |

â­ = Low, â­â­â­â­ = High

---

## ğŸ”§ Quick Examples

### Train with DPO (Most Common)
```python
from trl import DPOTrainer, DPOConfig
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model (already SFT'd)
model = AutoModelForCausalLM.from_pretrained("my-sft-model")
tokenizer = AutoTokenizer.from_pretrained("my-sft-model")

# Load preference data
dataset = load_dataset("trl-lib/ultrafeedback_binarized")

# Configure
config = DPOConfig(
    beta=0.1,
    learning_rate=5e-7,
    per_device_train_batch_size=4,
)

# Train
trainer = DPOTrainer(
    model=model,
    args=config,
    train_dataset=dataset["train"],
    processing_class=tokenizer,
)
trainer.train()
```

### Memory-Optimized Training (24GB GPU)
```python
config = DPOConfig(
    per_device_train_batch_size=1,       # Minimum
    gradient_accumulation_steps=16,      # Effective batch=16
    gradient_checkpointing=True,         # Save ~40% memory
    precompute_ref_log_probs=True,       # No ref model in memory
    bf16=True,                           # Mixed precision
)

# Add LoRA for even more savings
from peft import LoraConfig
peft_config = LoraConfig(r=16, lora_alpha=32)
trainer = DPOTrainer(..., peft_config=peft_config)
```

**See**: Each `*_training_ANNOTATED.py` script for full examples

---

## ğŸ“ˆ Monitor Your Training

### Key Metrics to Watch

**DPO/ORPO/KTO/CPO:**
- âœ… `loss` decreasing
- âœ… `rewards/margins` > 0 and increasing
- âœ… `rewards/accuracies` > 0.7
- âš ï¸ If margins negative â†’ increase beta

**PPO/RLOO/GRPO:**
- âœ… `rewards/mean` increasing
- âœ… `objective/kl` < 0.5 (not exploding)
- âœ… `entropy` not collapsing
- âš ï¸ If KL high â†’ increase beta or reduce LR

---

## ğŸ› Common Issues & Solutions

### Out of Memory
```python
# Try these in order:
1. batch_size = 1
2. gradient_checkpointing = True
3. precompute_ref_log_probs = True  # DPO/KTO
4. Use LoRA/PEFT
5. max_length = 512
```

### Loss Not Decreasing
```python
# Try:
1. Increase learning_rate to 1e-6
2. Increase beta to 0.2-0.3
3. Check data quality
4. Try different loss_type
```

### Model Outputs Gibberish
```python
# Probably:
1. Learning rate too high â†’ reduce to 1e-7
2. Too many epochs â†’ reduce to 1
3. Beta too high â†’ try 0.1
```

**Full troubleshooting**: See each training script

---

## ğŸ“š Additional Resources

### Official Docs
- **TRL Docs**: https://huggingface.co/docs/trl
- **GitHub**: https://github.com/huggingface/trl

### Papers
- **DPO**: [Rafailov et al., 2023](https://arxiv.org/abs/2305.18290)
- **PPO**: [Schulman et al., 2017](https://arxiv.org/abs/1707.06347)
- **KTO**: [Ethayarajh et al., 2024](https://arxiv.org/abs/2402.01306)
- **ORPO**: [Hong et al., 2024](https://arxiv.org/abs/2403.07691)
- **RLOO**: [Ahmadian et al., 2024](https://arxiv.org/abs/2402.14740)

---

## ğŸ™ Acknowledgments

- **TRL Team** at Hugging Face
- **Original paper authors**
- **Open source community**

---

**Last Updated**: 2025-11-27
**Status**: 9/15 trainers (95%+ use cases covered)
**License**: Apache 2.0 (same as TRL)

**Ready to start?** Pick a path above and dive in! ğŸš€
